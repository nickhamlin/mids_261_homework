{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##This is an example of reducer side inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Write two tables into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting customers.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile customers.dat\n",
    "1|Alice Bob|31|CA\n",
    "2|Sam Sneed|51|NV\n",
    "3|Jon Sneed|37|CA\n",
    "4|Arnold Wesise|17|NY\n",
    "5|Henry Bob|25|NV\n",
    "6|Yo Yo Ma|37|NY\n",
    "7|Jon York|41|WA\n",
    "8|Alex Ball|26|WA\n",
    "9|Jim Davis|19|CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting orders.dat\n"
     ]
    }
   ],
   "source": [
    "%%writefile orders.dat\n",
    "1|Apple\n",
    "3|Garlic\n",
    "2|Milk\n",
    "1|Iphone\n",
    "4|Ipad\n",
    "5|Book\n",
    "7|Potato\n",
    "8|Tomato\n",
    "9|Orange\n",
    "5|shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MrJob class for ReducerSideInnerJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducersideinnerjoin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducersideinnerjoin.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from mrjob.compat import get_jobconf_value\n",
    " \n",
    "class innerjoin(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        x = line.split(\"|\")\n",
    "        if len(x) == 4:\n",
    "            yield x[0], (\"lefttable\", x[1], x[2], x[3])\n",
    "        else:\n",
    "            yield x[0], (\"righttable\", x[1])\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        customers = list()\n",
    "        orders = list()\n",
    "        for val in values:\n",
    "            if val[0] == u'lefttable':\n",
    "                customers.append(val)\n",
    "            else:\n",
    "                orders.append(val)\n",
    "        for o in orders:\n",
    "            for c in customers:\n",
    "                yield None, [key] + c[1:] + o[1:]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    innerjoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run the code through python driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Reminder: You cannot use the programmatic runner functionality in the same file as your job class. That is because the file with the job class is sent to Hadoop to be run. Therefore, the job file cannot attempt to start the Hadoop job, or you would be recursively creating Hadoop jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use make_runner() to run an MRJob\n",
    "1. seperate driver from mapreduce jobs\n",
    "2. now we can run it within pythonnode book \n",
    "3. In python, typically one class is in each file. Each mrjob job is a seperate class, should be in a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1', u'Alice Bob', u'31', u'CA', u'Apple']\n",
      "[u'1', u'Alice Bob', u'31', u'CA', u'Iphone']\n",
      "[u'2', u'Sam Sneed', u'51', u'NV', u'Milk']\n",
      "[u'3', u'Jon Sneed', u'37', u'CA', u'Garlic']\n",
      "[u'4', u'Arnold Wesise', u'17', u'NY', u'Ipad']\n",
      "[u'5', u'Henry Bob', u'25', u'NV', u'Book']\n",
      "[u'5', u'Henry Bob', u'25', u'NV', u'shoes']\n",
      "[u'7', u'Jon York', u'41', u'WA', u'Potato']\n",
      "[u'8', u'Alex Ball', u'26', u'WA', u'Tomato']\n",
      "[u'9', u'Jim Davis', u'19', u'CA', u'Orange']\n",
      "\n",
      "\n",
      "There are 10 records\n"
     ]
    }
   ],
   "source": [
    "from reducersideinnerjoin import innerjoin\n",
    "mr_job = innerjoin(args=['customers.dat','orders.dat'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    count = 0\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        key,value =  mr_job.parse_output_line(line)\n",
    "        print value\n",
    "        count = count + 1\n",
    "print \"\\n\"\n",
    "print \"There are %s records\" %count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
